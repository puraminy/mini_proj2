{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Dropout.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj2/blob/master/RNN_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2W_bevOEYdEv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load DATA\n"
      ]
    },
    {
      "metadata": {
        "id": "OrhkJ3uO5lD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data Main Source\n",
        "#####https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data"
      ]
    },
    {
      "metadata": {
        "id": "_RE5WNDFmq-T",
        "colab_type": "code",
        "outputId": "9bea73a3-e999-4f15-ee14-aa91315f390e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/puraminy/mini_proj2\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mini_proj2'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 100 (delta 40), reused 9 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (100/100), 3.29 MiB | 3.61 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyxKkShP1XJg",
        "colab_type": "code",
        "outputId": "5401ac6a-b2eb-4d09-9c94-262175450275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
        "\n",
        "#github = \"https://raw.githubusercontent.com/puraminy/mini_proj2/master/polution.csv\"\n",
        "\n",
        "github = \"mini_proj2/polution.csv\"\n",
        "\n",
        "c=pd.read_csv(github)\n",
        "data=np.asarray(c)\n",
        "print(data)\n",
        "np.shape(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.12977867 0.35294122 0.24590163 ... 0.00229001 0.         0.        ]\n",
            " [0.14889336 0.36764708 0.24590163 ... 0.00381099 0.         0.        ]\n",
            " [0.15995975 0.42647061 0.22950819 ... 0.00533197 0.         0.        ]\n",
            " ...\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.40558836 0.         0.        ]\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.41399646 0.         0.        ]\n",
            " [0.00804829 0.2647059  0.24590163 ... 0.42086649 0.         0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43799, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "kUWTG2AjYoJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "a4ZQnQPLYpOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "values = c.values\n",
        "# specify columns to plot\n",
        "def plot_trends():\n",
        "  groups = [0, 1, 2, 3, 5, 6, 7]\n",
        "  i = 1\n",
        "  # plot each column\n",
        "  pyplot.figure()\n",
        "  for group in groups:\n",
        "    pyplot.subplot(len(groups), 1, i)\n",
        "    pyplot.plot(values[:, group])\n",
        "    pyplot.title(c.columns[group], y=0.5, loc='right')\n",
        "    i += 1\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZmb0Vi6v3Y2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing Time Series"
      ]
    },
    {
      "metadata": {
        "id": "xKQRfONIv-Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "# load dataset\n",
        "def create_data(n_hours = 24, n_train = 10000):\n",
        "  dataset = read_csv('mini_proj2/polution.csv', header=0)\n",
        "  values = dataset.values\n",
        "  \n",
        "  print(np.shape(values))\n",
        "  # integer encode direction\n",
        "  encoder = LabelEncoder()\n",
        "  values[:,4] = encoder.fit_transform(values[:,4])\n",
        "  # ensure all data is float\n",
        "  values = values.astype('float32')\n",
        "  # normalize features\n",
        "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  scaled = values # scaler.fit_transform(values)\n",
        "  # specify the number of lag hours\n",
        "  # n_hours = 24\n",
        "  n_features = 8 \n",
        "  # frame as supervised learning\n",
        "  reframed = series_to_supervised(scaled, n_hours, 1)\n",
        "\n",
        " # print(reframed[:2])\n",
        " # print(reframed.shape)\n",
        "\n",
        "  # split into train and test sets\n",
        "  values = reframed.values\n",
        "   #365 * 24\n",
        "  train = values[:n_train, :]\n",
        "  test = values[n_train:, :]\n",
        "  # split into input and outputs\n",
        "  n_obs = n_hours * n_features\n",
        "  train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
        "\n",
        "  test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
        "  print(train_X.shape, len(train_X), train_y.shape)\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "    \n",
        "  train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "  test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B99o1xwiwadG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Train & Test Data"
      ]
    },
    {
      "metadata": {
        "id": "PVs3NEqA_SOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_data2(n_hours = 24, n_train = 10000, step =1):\n",
        "  n_train+=n_hours\n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = 1000 #data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "\n",
        "  train_X = np.zeros([n_train-n_hours, n_hours, 8])\n",
        "  train_y = np.zeros([n_train-n_hours, 1])\n",
        "  for i in range (n_train-n_hours):\n",
        "      train_X[i,:,:] = train[i:i+n_hours,:]\n",
        "      train_y[i] = train[i+n_hours,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_hours, n_hours, 8])\n",
        "  test_y = np.zeros([n_test-n_hours, 1])\n",
        "  for i in range (n_test-n_hours):\n",
        "      test_X[i,:,:] = test[i:i+n_hours,:]\n",
        "      test_y[i] = test[i+n_hours,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n",
        "def create_data3(n_records = 7, n_train = 10000, n_test = -1, stride =24):\n",
        "  n_train+=n_records*stride\n",
        "  if n_test > 0:\n",
        "    n_test+=n_records*stride\n",
        "  \n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = n_test if n_test > 0 else data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "  n_items = n_records*stride\n",
        "\n",
        "  train_X = np.zeros([n_train-n_items, n_records, 8])\n",
        "  train_y = np.zeros([n_train-n_items, 1])\n",
        "  \n",
        "  \n",
        "  for i in range (n_train-n_items):\n",
        "      train_X[i,:,:] = train[i:i+n_items:stride,:]\n",
        "      train_y[i] = train[i+n_items,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_items, n_records, 8])\n",
        "  test_y = np.zeros([n_test-n_items, 1])\n",
        "  for i in range (n_test-n_items):\n",
        "      test_X[i,:,:] = test[i:i+n_items:stride,:]\n",
        "      test_y[i] = test[i+n_items,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLRasYy_RTgT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "eZ_fxxL6xmSP",
        "colab_type": "code",
        "outputId": "ce65e13b-982e-4ac8-8abe-1d15dfd904ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1975
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU Processing\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at:{}'.format(device_name))\n",
        "\n",
        "def diffusion_model(left, right, optimizer):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Merge([left, right],mode='sum'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4))\n",
        "  model.add(Activation('linear'))\n",
        "  model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_model(cell_type = \"normal\", optimizer = 'adam', dropout=0):\n",
        "  if cell_type == \"lstm\":\n",
        "    model = Sequential()   \n",
        "    if dropout == 0:   \n",
        "      model.add(LSTM(24, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    else:\n",
        "      model.add(LSTM(24, batch_input_shape=(100, train_X.shape[1], train_X.shape[2]), \n",
        "                     stateful=True, \n",
        "                     dropout=dropout))\n",
        "      model.add(Dense(1, activation='linear'))\n",
        "  elif cell_type == \"gru\":\n",
        "    model = Sequential()\n",
        "    model.add(GRU(30, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  elif cell_type == \"rnn\":\n",
        "    model = Sequential()\n",
        "    if dropout == 0:\n",
        "      model.add(SimpleRNN(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    else:\n",
        "      model.add(SimpleRNN(100, batch_input_shape=(100, train_X.shape[1], train_X.shape[2]), \n",
        "                     stateful=True, \n",
        "                     dropout=dropout))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  else:\n",
        "    model = Sequential()\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10))\n",
        "    model.add(Dense(10)) \n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  \n",
        "  model.name = 'model_' + cell_type + '_' + optimizer + ('_dropout_' +str(dropout) if dropout else '')\n",
        "  if optimizer == 'rmsprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "    \n",
        "#   model.compile(loss='mae', optimizer=optimizer)\n",
        "  \n",
        "  model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "n_train = 10000\n",
        "n_test = 25000\n",
        "n_hours = 24\n",
        "\n",
        "# train_X,train_y, test_X, test_y = create_data2(n_hours, n_train=n_train)\n",
        "train_X,train_y, test_X, test_y = create_data3(n_records=4, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=24)\n",
        "\n",
        "\n",
        "model_normal_adam = create_model(optimizer='adam') #0\n",
        "model_normal_sgd = create_model(optimizer='sgd') #1\n",
        "model_normal_rmsprop = create_model(optimizer='rmsprop') #2\n",
        "\n",
        "model_lstm_adam = create_model(\"lstm\", 'adam') #3\n",
        "model_lstm_sgd = create_model(\"lstm\",'sgd') #4\n",
        "model_lstm_rmsprop = create_model(\"lstm\",'rmsprop') #5\n",
        "\n",
        "\n",
        "model_gru_adam = create_model(\"gru\", 'adam') #6\n",
        "model_gru_sgd = create_model(\"gru\", 'sgd') #7\n",
        "model_gru_rmsprop = create_model(\"gru\", 'rmsprop') #8\n",
        "\n",
        "\n",
        "model_rnn_adam = create_model(\"rnn\", 'adam') #9\n",
        "model_rnn_sgd = create_model(\"rnn\", 'sgd') #10\n",
        "model_rnn_rmsprop = create_model(\"rnn\", 'rmsprop') #11\n",
        "\n",
        "model_lstm_adam_dropout_2 = create_model(\"lstm\", 'adam',dropout=0.2) #12\n",
        "model_lstm_adam_dropout_4 = create_model(\"lstm\", 'adam',dropout=0.4) #13\n",
        "model_lstm_adam_dropout_6 = create_model(\"lstm\", 'adam',dropout=0.6) #14\n",
        "\n",
        "model_rnn_adam_dropout_2 = create_model(\"rnn\", 'adam',dropout=0.2) #15\n",
        "model_rnn_adam_dropout_4 = create_model(\"rnn\", 'adam',dropout=0.4) #16\n",
        "\n",
        "\n",
        "\n",
        "models = [model_normal_adam, model_normal_sgd, model_normal_rmsprop,\n",
        "          model_lstm_adam, model_lstm_sgd, model_lstm_rmsprop,\n",
        "          model_gru_adam,model_gru_sgd, model_gru_rmsprop,\n",
        "          model_rnn_adam, model_rnn_sgd, model_rnn_rmsprop,\n",
        "          model_lstm_adam_dropout_2, model_lstm_adam_dropout_4,model_lstm_adam_dropout_6,\n",
        "          model_rnn_adam_dropout_2, model_rnn_adam_dropout_4]\n",
        "\n",
        "\n",
        "Run = [0]*len(models)\n",
        "Run[0] = 0\n",
        "Run[12] = 0\n",
        "Run[13] = 0\n",
        "Run[14] = 1\n",
        "Run[15] = 1\n",
        "\n",
        "EPOCHS =[30]*len(models)\n",
        "model_index = 0\n",
        "\n",
        "batch_size =100\n",
        "\n",
        "legends = []\n",
        "\n",
        "for model in models:\n",
        "\n",
        "  # fit network\n",
        "  model_name = model.name\n",
        "  print(f\"##################### Model {model_name} Hours {n_hours} ##################\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  if not Run[model_index]:\n",
        "    print(\"Skipped\")\n",
        "    model_index+=1\n",
        "    continue\n",
        "\n",
        "  legends.append(model_name)\n",
        "\n",
        "  history = model.fit(train_X, train_y, \n",
        "                      epochs=EPOCHS[model_index], batch_size=batch_size, \n",
        "#                       validation_data=(test_X, test_y), \n",
        "                      validation_split=0.1,\n",
        "                      verbose=1, shuffle=True)\n",
        "  # plot history\n",
        "\n",
        "  plt.figure(0)\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.xlabel(\"Num of Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Training\")\n",
        "  plt.legend(legends)\n",
        "\n",
        "  plt.figure(1)\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.xlabel(\"Num of Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Test\")\n",
        "  plt.legend(legends)\n",
        "  \n",
        "  \n",
        "  test_loss = model.evaluate(test_X, test_y,batch_size=batch_size)\n",
        "  \n",
        "  print(\"test loss:\",test_loss)\n",
        "\n",
        "  test_predictions = model.predict(test_X,batch_size=batch_size).flatten()\n",
        " \n",
        "\n",
        "\n",
        "  plt.figure(num=model_index+2, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "  plt.title(f\"Prediction of {model_name}: Test loss (MSE, MAE): {test_loss}\")\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('normalized value')\n",
        "  plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "  plt.plot(data[:,0], 'bo')\n",
        "  plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go', alpha=0.4)\n",
        "  plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "#   plt.savefig(model_name)\n",
        "\n",
        "  model_index +=1\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at:/device:GPU:0\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 4, 8) (10000, 1) (25000, 4, 8) (25000, 1)\n",
            "##################### Model model_normal_adam Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_normal_sgd Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_normal_rmsprop Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_sgd Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_rmsprop Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_gru_adam Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_gru_sgd Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_gru_rmsprop Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_adam Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_sgd Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_rmsprop Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_dropout_0.2 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_dropout_0.4 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_dropout_0.6 Hours 24 ##################\n",
            "\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/30\n",
            "9000/9000 [==============================] - 5s 593us/step - loss: 0.0101 - mean_absolute_error: 0.0756 - val_loss: 0.0154 - val_mean_absolute_error: 0.0848\n",
            "Epoch 2/30\n",
            "9000/9000 [==============================] - 2s 261us/step - loss: 0.0086 - mean_absolute_error: 0.0697 - val_loss: 0.0149 - val_mean_absolute_error: 0.0883\n",
            "Epoch 3/30\n",
            "9000/9000 [==============================] - 2s 254us/step - loss: 0.0084 - mean_absolute_error: 0.0689 - val_loss: 0.0143 - val_mean_absolute_error: 0.0902\n",
            "Epoch 4/30\n",
            "9000/9000 [==============================] - 2s 264us/step - loss: 0.0084 - mean_absolute_error: 0.0688 - val_loss: 0.0146 - val_mean_absolute_error: 0.0861\n",
            "Epoch 5/30\n",
            "9000/9000 [==============================] - 2s 269us/step - loss: 0.0083 - mean_absolute_error: 0.0683 - val_loss: 0.0141 - val_mean_absolute_error: 0.0867\n",
            "Epoch 6/30\n",
            "9000/9000 [==============================] - 2s 261us/step - loss: 0.0081 - mean_absolute_error: 0.0677 - val_loss: 0.0150 - val_mean_absolute_error: 0.0830\n",
            "Epoch 7/30\n",
            "9000/9000 [==============================] - 2s 270us/step - loss: 0.0082 - mean_absolute_error: 0.0681 - val_loss: 0.0145 - val_mean_absolute_error: 0.0834\n",
            "Epoch 8/30\n",
            "9000/9000 [==============================] - 2s 261us/step - loss: 0.0080 - mean_absolute_error: 0.0671 - val_loss: 0.0143 - val_mean_absolute_error: 0.0963\n",
            "Epoch 9/30\n",
            "9000/9000 [==============================] - 2s 270us/step - loss: 0.0082 - mean_absolute_error: 0.0679 - val_loss: 0.0141 - val_mean_absolute_error: 0.0823\n",
            "Epoch 10/30\n",
            "9000/9000 [==============================] - 2s 262us/step - loss: 0.0080 - mean_absolute_error: 0.0668 - val_loss: 0.0140 - val_mean_absolute_error: 0.0840\n",
            "Epoch 11/30\n",
            "9000/9000 [==============================] - 2s 265us/step - loss: 0.0079 - mean_absolute_error: 0.0665 - val_loss: 0.0139 - val_mean_absolute_error: 0.0855\n",
            "Epoch 12/30\n",
            "9000/9000 [==============================] - 2s 265us/step - loss: 0.0081 - mean_absolute_error: 0.0673 - val_loss: 0.0141 - val_mean_absolute_error: 0.0832\n",
            "Epoch 13/30\n",
            "9000/9000 [==============================] - 2s 263us/step - loss: 0.0080 - mean_absolute_error: 0.0668 - val_loss: 0.0139 - val_mean_absolute_error: 0.0923\n",
            "Epoch 14/30\n",
            "9000/9000 [==============================] - 2s 265us/step - loss: 0.0081 - mean_absolute_error: 0.0674 - val_loss: 0.0139 - val_mean_absolute_error: 0.0825\n",
            "Epoch 15/30\n",
            "9000/9000 [==============================] - 2s 264us/step - loss: 0.0080 - mean_absolute_error: 0.0670 - val_loss: 0.0138 - val_mean_absolute_error: 0.0893\n",
            "Epoch 16/30\n",
            "9000/9000 [==============================] - 2s 274us/step - loss: 0.0079 - mean_absolute_error: 0.0665 - val_loss: 0.0135 - val_mean_absolute_error: 0.0839\n",
            "Epoch 17/30\n",
            "9000/9000 [==============================] - 2s 275us/step - loss: 0.0080 - mean_absolute_error: 0.0669 - val_loss: 0.0137 - val_mean_absolute_error: 0.0830\n",
            "Epoch 18/30\n",
            "9000/9000 [==============================] - 2s 263us/step - loss: 0.0079 - mean_absolute_error: 0.0667 - val_loss: 0.0138 - val_mean_absolute_error: 0.0837\n",
            "Epoch 19/30\n",
            "9000/9000 [==============================] - 2s 267us/step - loss: 0.0078 - mean_absolute_error: 0.0663 - val_loss: 0.0143 - val_mean_absolute_error: 0.0818\n",
            "Epoch 20/30\n",
            "9000/9000 [==============================] - 2s 273us/step - loss: 0.0079 - mean_absolute_error: 0.0665 - val_loss: 0.0142 - val_mean_absolute_error: 0.0832\n",
            "Epoch 21/30\n",
            "9000/9000 [==============================] - 2s 267us/step - loss: 0.0079 - mean_absolute_error: 0.0661 - val_loss: 0.0136 - val_mean_absolute_error: 0.0833\n",
            "Epoch 22/30\n",
            "9000/9000 [==============================] - 2s 257us/step - loss: 0.0078 - mean_absolute_error: 0.0658 - val_loss: 0.0135 - val_mean_absolute_error: 0.0861\n",
            "Epoch 23/30\n",
            "9000/9000 [==============================] - 2s 272us/step - loss: 0.0078 - mean_absolute_error: 0.0659 - val_loss: 0.0145 - val_mean_absolute_error: 0.0794\n",
            "Epoch 24/30\n",
            "9000/9000 [==============================] - 2s 264us/step - loss: 0.0079 - mean_absolute_error: 0.0665 - val_loss: 0.0137 - val_mean_absolute_error: 0.0821\n",
            "Epoch 25/30\n",
            "4400/9000 [=============>................] - ETA: 1s - loss: 0.0077 - mean_absolute_error: 0.0651"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TPwH-yrYBQrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "\n",
        "|hours|Mean Absolute Error |  loss function \n",
        "|--|--|\n",
        "|1  |  0.01328097059825935\n",
        "|10 |   0.016554366017757748 \n",
        "| 24 | 0.02725420025859075 \n",
        "| 50|  0.023748632413918377\n",
        "| 50 |    0.03477914220943476 | MSE\n",
        "| 1 |  0.042994678616862125\n",
        "| 1 |  0.016500973061146625\n",
        "| 48| 0.02513334271283482 \n",
        "|48| 0.01888059062355331\n",
        "|72| 0.020766058881570845\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cBWrhC9hRS4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "072c3500-6f3c-4d10-9f98-0f230119d6eb"
      },
      "cell_type": "code",
      "source": [
        "|# import pandas as pd\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "# y_train = train_y\n",
        "# x_train = train_X\n",
        "\n",
        "# y_test = test_y\n",
        "# x_test = test_X\n",
        "\n",
        "# EPOCHS = 10\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(120, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(18, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(1, activation='linear')\n",
        "# ])\n",
        "\n",
        "# optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "# model.compile(loss='mse',\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=['mae'])\n",
        "\n",
        "\n",
        "# # Store training stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# def plot_history(history):\n",
        "#     plt.figure()\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Mean Abs Error ')\n",
        "#     plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "#              label='Train Loss')\n",
        "#     plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "#              label='Val loss')\n",
        "#     plt.legend()\n",
        "#     #plt.ylim([0, 0.2])\n",
        "\n",
        "\n",
        "# #model.summary()\n",
        "# history = model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "#                 validation_split=0.1, verbose=1)\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# plot_history(history)\n",
        "# test_predictions = model.predict(x_test).flatten()\n",
        "# test_acc, test_loss = model.evaluate(x_test, y_test)\n",
        "# print(f\"test accuracy: {test_acc}, test loss {test_loss}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d37156ee0b35>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    |# import pandas as pd\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IUwrIvafdh4K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Plot Predicted Values"
      ]
    },
    {
      "metadata": {
        "id": "WKBAT0RFZCDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(num=None, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('normalized value')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "# plt.plot(data[:,0], 'bo')\n",
        "# plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}