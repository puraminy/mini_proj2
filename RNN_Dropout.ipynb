{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Dropout.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj2/blob/master/RNN_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2W_bevOEYdEv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load DATA\n"
      ]
    },
    {
      "metadata": {
        "id": "OrhkJ3uO5lD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data Main Source\n",
        "#####https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data"
      ]
    },
    {
      "metadata": {
        "id": "_RE5WNDFmq-T",
        "colab_type": "code",
        "outputId": "06925415-5597-4d94-94e4-4b76ddb67ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/puraminy/mini_proj2\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mini_proj2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyxKkShP1XJg",
        "colab_type": "code",
        "outputId": "310d5307-c20a-4a2b-f961-40abb5a96099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
        "\n",
        "#github = \"https://raw.githubusercontent.com/puraminy/mini_proj2/master/polution.csv\"\n",
        "\n",
        "github = \"mini_proj2/polution.csv\"\n",
        "\n",
        "c=pd.read_csv(github)\n",
        "data=np.asarray(c)\n",
        "print(data)\n",
        "np.shape(data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.12977867 0.35294122 0.24590163 ... 0.00229001 0.         0.        ]\n",
            " [0.14889336 0.36764708 0.24590163 ... 0.00381099 0.         0.        ]\n",
            " [0.15995975 0.42647061 0.22950819 ... 0.00533197 0.         0.        ]\n",
            " ...\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.40558836 0.         0.        ]\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.41399646 0.         0.        ]\n",
            " [0.00804829 0.2647059  0.24590163 ... 0.42086649 0.         0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43799, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "kUWTG2AjYoJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "a4ZQnQPLYpOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "values = c.values\n",
        "# specify columns to plot\n",
        "def plot_trends():\n",
        "  groups = [0, 1, 2, 3, 5, 6, 7]\n",
        "  i = 1\n",
        "  # plot each column\n",
        "  pyplot.figure()\n",
        "  for group in groups:\n",
        "    pyplot.subplot(len(groups), 1, i)\n",
        "    pyplot.plot(values[:, group])\n",
        "    pyplot.title(c.columns[group], y=0.5, loc='right')\n",
        "    i += 1\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZmb0Vi6v3Y2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing Time Series"
      ]
    },
    {
      "metadata": {
        "id": "xKQRfONIv-Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "# load dataset\n",
        "def create_data(n_hours = 24, n_train = 10000):\n",
        "  dataset = read_csv('mini_proj2/polution.csv', header=0)\n",
        "  values = dataset.values\n",
        "  \n",
        "  print(np.shape(values))\n",
        "  # integer encode direction\n",
        "  encoder = LabelEncoder()\n",
        "  values[:,4] = encoder.fit_transform(values[:,4])\n",
        "  # ensure all data is float\n",
        "  values = values.astype('float32')\n",
        "  # normalize features\n",
        "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  scaled = values # scaler.fit_transform(values)\n",
        "  # specify the number of lag hours\n",
        "  # n_hours = 24\n",
        "  n_features = 8 \n",
        "  # frame as supervised learning\n",
        "  reframed = series_to_supervised(scaled, n_hours, 1)\n",
        "\n",
        " # print(reframed[:2])\n",
        " # print(reframed.shape)\n",
        "\n",
        "  # split into train and test sets\n",
        "  values = reframed.values\n",
        "   #365 * 24\n",
        "  train = values[:n_train, :]\n",
        "  test = values[n_train:, :]\n",
        "  # split into input and outputs\n",
        "  n_obs = n_hours * n_features\n",
        "  train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
        "\n",
        "  test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
        "  print(train_X.shape, len(train_X), train_y.shape)\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "    \n",
        "  train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "  test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B99o1xwiwadG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Train & Test Data"
      ]
    },
    {
      "metadata": {
        "id": "PVs3NEqA_SOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_data2(n_hours = 24, n_train = 10000, step =1):\n",
        "  n_train+=n_hours\n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = 1000 #data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "\n",
        "  train_X = np.zeros([n_train-n_hours, n_hours, 8])\n",
        "  train_y = np.zeros([n_train-n_hours, 1])\n",
        "  for i in range (n_train-n_hours):\n",
        "      train_X[i,:,:] = train[i:i+n_hours,:]\n",
        "      train_y[i] = train[i+n_hours,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_hours, n_hours, 8])\n",
        "  test_y = np.zeros([n_test-n_hours, 1])\n",
        "  for i in range (n_test-n_hours):\n",
        "      test_X[i,:,:] = test[i:i+n_hours,:]\n",
        "      test_y[i] = test[i+n_hours,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n",
        "def create_data3(n_records = 7, n_train = 10000, n_test = -1, stride =24):\n",
        "  n_train+=n_records*stride\n",
        "  if n_test > 0:\n",
        "    n_test+=n_records*stride\n",
        "  \n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = n_test if n_test > 0 else data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "  n_items = n_records*stride\n",
        "\n",
        "  train_X = np.zeros([n_train-n_items, n_records, 8])\n",
        "  train_y = np.zeros([n_train-n_items, 1])\n",
        "  \n",
        "  \n",
        "  for i in range (n_train-n_items):\n",
        "      train_X[i,:,:] = train[i:i+n_items:stride,:]\n",
        "      train_y[i] = train[i+n_items,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_items, n_records, 8])\n",
        "  test_y = np.zeros([n_test-n_items, 1])\n",
        "  for i in range (n_test-n_items):\n",
        "      test_X[i,:,:] = test[i:i+n_items:stride,:]\n",
        "      test_y[i] = test[i+n_items,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLRasYy_RTgT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "eZ_fxxL6xmSP",
        "colab_type": "code",
        "outputId": "434d5b3a-9922-4d46-9d7c-9bd6f3376ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2309
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU Processing\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at:{}'.format(device_name))\n",
        "\n",
        "def diffusion_model(left, right, optimizer):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Merge([left, right],mode='sum'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4))\n",
        "  model.add(Activation('linear'))\n",
        "  model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_model(cell_type = \"normal\", optimizer = 'adam', loss_func='mse', dropout=0):\n",
        "  if cell_type == \"lstm\":\n",
        "    model = Sequential()   \n",
        "    if dropout == 0:   \n",
        "      model.add(LSTM(24, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    else:\n",
        "      model.add(LSTM(24, batch_input_shape=(100, train_X.shape[1], train_X.shape[2]), \n",
        "                     stateful=True, \n",
        "                     dropout=dropout))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  elif cell_type == \"gru\":\n",
        "    model = Sequential()\n",
        "    model.add(GRU(30, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  elif cell_type == \"rnn\":\n",
        "    model = Sequential()\n",
        "    if dropout == 0:\n",
        "      model.add(SimpleRNN(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    else:\n",
        "      model.add(SimpleRNN(100, batch_input_shape=(100, train_X.shape[1], train_X.shape[2]), \n",
        "                     stateful=True, \n",
        "                     dropout=dropout))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  else:\n",
        "    model = Sequential()\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10))\n",
        "    model.add(Dense(10)) \n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  \n",
        "  model.name = 'model_' + cell_type + '_' + optimizer + '_' +loss_func + ('_dropout_' +str(dropout) if dropout else '')\n",
        "  if optimizer == 'rmsprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "    \n",
        "#   model.compile(loss='mae', optimizer=optimizer)\n",
        "  \n",
        "  model.compile(loss=loss_func,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "n_train = 10000\n",
        "n_test = 25000\n",
        "n_hours = 24\n",
        "\n",
        "# train_X,train_y, test_X, test_y = create_data2(n_hours, n_train=n_train)\n",
        "train_X,train_y, test_X, test_y = create_data3(n_records=24, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=1)\n",
        "\n",
        "\n",
        "model_normal_adam = create_model(optimizer='adam') #0\n",
        "model_normal_sgd = create_model(optimizer='sgd') #1\n",
        "model_normal_rmsprop = create_model(optimizer='rmsprop') #2\n",
        "\n",
        "model_lstm_adam = create_model(\"lstm\", 'adam') #3\n",
        "model_lstm_sgd = create_model(\"lstm\",'sgd') #4\n",
        "model_lstm_rmsprop = create_model(\"lstm\",'rmsprop') #5\n",
        "\n",
        "\n",
        "model_gru_adam = create_model(\"gru\", 'adam') #6\n",
        "model_gru_sgd = create_model(\"gru\", 'sgd') #7\n",
        "model_gru_rmsprop = create_model(\"gru\", 'rmsprop') #8\n",
        "\n",
        "\n",
        "model_rnn_adam = create_model(\"rnn\", 'adam') #9\n",
        "model_rnn_sgd = create_model(\"rnn\", 'sgd') #10\n",
        "model_rnn_rmsprop = create_model(\"rnn\", 'rmsprop') #11\n",
        "\n",
        "model_lstm_adam_dropout_2 = create_model(\"lstm\", 'adam',dropout=0.2) #12\n",
        "model_lstm_adam_dropout_4 = create_model(\"lstm\", 'adam',dropout=0.4) #13\n",
        "model_lstm_adam_dropout_6 = create_model(\"lstm\", 'adam',dropout=0.6) #14\n",
        "\n",
        "model_rnn_adam_dropout_2 = create_model(\"rnn\", 'adam',dropout=0.2) #15\n",
        "model_rnn_adam_dropout_4 = create_model(\"rnn\", 'adam',dropout=0.4) #16\n",
        "\n",
        "model_lstm_adam_mse = create_model(\"lstm\", 'adam', 'mse') #17\n",
        "model_lstm_adam_mae = create_model(\"lstm\",'sgd', 'mae') #18\n",
        "model_lstm_adam_logcosh = create_model(\"lstm\",'rmsprop', 'logcosh') #19\n",
        "\n",
        "\n",
        "\n",
        "models = [model_normal_adam, model_normal_sgd, model_normal_rmsprop,\n",
        "          model_lstm_adam, model_lstm_sgd, model_lstm_rmsprop,\n",
        "          model_gru_adam,model_gru_sgd, model_gru_rmsprop,\n",
        "          model_rnn_adam, model_rnn_sgd, model_rnn_rmsprop,\n",
        "          model_lstm_adam_dropout_2, model_lstm_adam_dropout_4,model_lstm_adam_dropout_6,\n",
        "          model_rnn_adam_dropout_2, model_rnn_adam_dropout_4,\n",
        "          model_lstm_adam_mse, model_lstm_adam_mae,\n",
        "          model_lstm_adam_logcosh]\n",
        "\n",
        "\n",
        "Run = [0]*len(models)\n",
        "Run[0] = 0\n",
        "Run[12] = 0\n",
        "Run[13] = 0\n",
        "Run[17] = 1\n",
        "Run[18] = 1\n",
        "Run[19] = 1\n",
        "\n",
        "EPOCHS =[24]*len(models)\n",
        "model_index = 0\n",
        "\n",
        "batch_size =100\n",
        "\n",
        "legends = []\n",
        "\n",
        "for model in models:\n",
        "\n",
        "  # fit network\n",
        "  model_name = model.name\n",
        "  print(f\"##################### Model {model_name} Hours {n_hours} ##################\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  if not Run[model_index]:\n",
        "    print(\"Skipped\")\n",
        "    model_index+=1\n",
        "    continue\n",
        "\n",
        "  legends.append(model_name)\n",
        "\n",
        "  history = model.fit(train_X, train_y, \n",
        "                      epochs=EPOCHS[model_index], \n",
        "#                       batch_size=batch_size, \n",
        "#                       validation_data=(test_X, test_y), \n",
        "                      validation_split=0.1,\n",
        "                      verbose=1, shuffle=True)\n",
        "  # plot history\n",
        "\n",
        "  plt.figure(0)\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.xlabel(\"Num of Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Training\")\n",
        "  plt.legend(legends)\n",
        "\n",
        "  plt.figure(1)\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.xlabel(\"Num of Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Test\")\n",
        "  plt.legend(legends)\n",
        "  \n",
        "  \n",
        "  test_loss = model.evaluate(test_X, test_y,batch_size=batch_size)\n",
        "  \n",
        "  print(\"test loss:\",test_loss)\n",
        "\n",
        "  test_predictions = model.predict(test_X,batch_size=batch_size).flatten()\n",
        " \n",
        "\n",
        "\n",
        "  plt.figure(num=model_index+2, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "  plt.title(f\"Prediction of {model_name}: Test loss (MSE, MAE): {test_loss}\")\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('normalized value')\n",
        "  plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "  plt.plot(data[:,0], 'bo')\n",
        "  plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go', alpha=0.4)\n",
        "  plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "#   plt.savefig(model_name)\n",
        "\n",
        "  model_index +=1\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at:/device:GPU:0\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 24, 8) (10000, 1) (25000, 24, 8) (25000, 1)\n",
            "##################### Model model_normal_adam_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_normal_sgd_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_normal_rmsprop_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_sgd_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_rmsprop_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_gru_adam_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_gru_sgd_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_gru_rmsprop_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_adam_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_sgd_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_rmsprop_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_mse_dropout_0.2 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_mse_dropout_0.4 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_mse_dropout_0.6 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_adam_mse_dropout_0.2 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_rnn_adam_mse_dropout_0.4 Hours 24 ##################\n",
            "\n",
            "\n",
            "Skipped\n",
            "##################### Model model_lstm_adam_mse Hours 24 ##################\n",
            "\n",
            "\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4ec803e2a32d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m#                       validation_data=(test_X, test_y),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                       \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                       verbose=1, shuffle=True)\n\u001b[0m\u001b[1;32m    156\u001b[0m   \u001b[0;31m# plot history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [100] vs. [32]\n\t [[{{node training_15/Adam/gradients/loss_257/dense_371_loss/mul_grad/Mul}} = Mul[T=DT_FLOAT, _class=[\"loc:@training_15/Adam/gradients/loss_257/dense_371_loss/mul_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_15/Adam/gradients/loss_257/dense_371_loss/truediv_grad/Reshape, _arg_dense_371_sample_weights_0_2/_6377)]]\n\t [[{{node loss_257/mul/_6397}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1117_loss_257/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TPwH-yrYBQrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "\n",
        "|hours|Mean Absolute Error |  loss function \n",
        "|--|--|\n",
        "|1  |  0.01328097059825935\n",
        "|10 |   0.016554366017757748 \n",
        "| 24 | 0.02725420025859075 \n",
        "| 50|  0.023748632413918377\n",
        "| 50 |    0.03477914220943476 | MSE\n",
        "| 1 |  0.042994678616862125\n",
        "| 1 |  0.016500973061146625\n",
        "| 48| 0.02513334271283482 \n",
        "|48| 0.01888059062355331\n",
        "|72| 0.020766058881570845\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cBWrhC9hRS4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "|# import pandas as pd\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "# y_train = train_y\n",
        "# x_train = train_X\n",
        "\n",
        "# y_test = test_y\n",
        "# x_test = test_X\n",
        "\n",
        "# EPOCHS = 10\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(120, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(18, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(1, activation='linear')\n",
        "# ])\n",
        "\n",
        "# optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "# model.compile(loss='mse',\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=['mae'])\n",
        "\n",
        "\n",
        "# # Store training stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# def plot_history(history):\n",
        "#     plt.figure()\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Mean Abs Error ')\n",
        "#     plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "#              label='Train Loss')\n",
        "#     plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "#              label='Val loss')\n",
        "#     plt.legend()\n",
        "#     #plt.ylim([0, 0.2])\n",
        "\n",
        "\n",
        "# #model.summary()\n",
        "# history = model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "#                 validation_split=0.1, verbose=1)\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# plot_history(history)\n",
        "# test_predictions = model.predict(x_test).flatten()\n",
        "# test_acc, test_loss = model.evaluate(x_test, y_test)\n",
        "# print(f\"test accuracy: {test_acc}, test loss {test_loss}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUwrIvafdh4K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Plot Predicted Values"
      ]
    },
    {
      "metadata": {
        "id": "WKBAT0RFZCDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(num=None, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('normalized value')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "# plt.plot(data[:,0], 'bo')\n",
        "# plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}