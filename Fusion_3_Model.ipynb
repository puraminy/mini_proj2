{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fusion_3 Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj2/blob/master/Fusion_3_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2W_bevOEYdEv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load DATA\n"
      ]
    },
    {
      "metadata": {
        "id": "OrhkJ3uO5lD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data Main Source\n",
        "#####https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data"
      ]
    },
    {
      "metadata": {
        "id": "_RE5WNDFmq-T",
        "colab_type": "code",
        "outputId": "d0001162-cbed-415a-f432-116b2adbfd04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/puraminy/mini_proj2\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mini_proj2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyxKkShP1XJg",
        "colab_type": "code",
        "outputId": "e3b4e86c-da6d-4978-e919-b0d9d83513a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
        "\n",
        "#github = \"https://raw.githubusercontent.com/puraminy/mini_proj2/master/polution.csv\"\n",
        "\n",
        "github = \"mini_proj2/polution.csv\"\n",
        "\n",
        "c=pd.read_csv(github)\n",
        "data=np.asarray(c)\n",
        "print(data)\n",
        "np.shape(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.12977867 0.35294122 0.24590163 ... 0.00229001 0.         0.        ]\n",
            " [0.14889336 0.36764708 0.24590163 ... 0.00381099 0.         0.        ]\n",
            " [0.15995975 0.42647061 0.22950819 ... 0.00533197 0.         0.        ]\n",
            " ...\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.40558836 0.         0.        ]\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.41399646 0.         0.        ]\n",
            " [0.00804829 0.2647059  0.24590163 ... 0.42086649 0.         0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43799, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "kUWTG2AjYoJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "a4ZQnQPLYpOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "values = c.values\n",
        "# specify columns to plot\n",
        "def plot_trends():\n",
        "  groups = [0, 1, 2, 3, 5, 6, 7]\n",
        "  i = 1\n",
        "  # plot each column\n",
        "  pyplot.figure()\n",
        "  for group in groups:\n",
        "    pyplot.subplot(len(groups), 1, i)\n",
        "    pyplot.plot(values[:, group])\n",
        "    pyplot.title(c.columns[group], y=0.5, loc='right')\n",
        "    i += 1\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZmb0Vi6v3Y2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing Time Series"
      ]
    },
    {
      "metadata": {
        "id": "xKQRfONIv-Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "# load dataset\n",
        "def create_data(n_hours = 24, n_train = 10000):\n",
        "  dataset = read_csv('mini_proj2/polution.csv', header=0)\n",
        "  values = dataset.values\n",
        "  \n",
        "  print(np.shape(values))\n",
        "  # integer encode direction\n",
        "  encoder = LabelEncoder()\n",
        "  values[:,4] = encoder.fit_transform(values[:,4])\n",
        "  # ensure all data is float\n",
        "  values = values.astype('float32')\n",
        "  # normalize features\n",
        "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  scaled = values # scaler.fit_transform(values)\n",
        "  # specify the number of lag hours\n",
        "  # n_hours = 24\n",
        "  n_features = 8 \n",
        "  # frame as supervised learning\n",
        "  reframed = series_to_supervised(scaled, n_hours, 1)\n",
        "\n",
        " # print(reframed[:2])\n",
        " # print(reframed.shape)\n",
        "\n",
        "  # split into train and test sets\n",
        "  values = reframed.values\n",
        "   #365 * 24\n",
        "  train = values[:n_train, :]\n",
        "  test = values[n_train:, :]\n",
        "  # split into input and outputs\n",
        "  n_obs = n_hours * n_features\n",
        "  train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
        "\n",
        "  test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
        "  print(train_X.shape, len(train_X), train_y.shape)\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "    \n",
        "  train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "  test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B99o1xwiwadG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Train & Test Data"
      ]
    },
    {
      "metadata": {
        "id": "PVs3NEqA_SOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_data2(n_hours = 24, n_train = 10000, step =1):\n",
        "  n_train+=n_hours\n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = 1000 #data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "\n",
        "  train_X = np.zeros([n_train-n_hours, n_hours, 8])\n",
        "  train_y = np.zeros([n_train-n_hours, 1])\n",
        "  for i in range (n_train-n_hours):\n",
        "      train_X[i,:,:] = train[i:i+n_hours,:]\n",
        "      train_y[i] = train[i+n_hours,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_hours, n_hours, 8])\n",
        "  test_y = np.zeros([n_test-n_hours, 1])\n",
        "  for i in range (n_test-n_hours):\n",
        "      test_X[i,:,:] = test[i:i+n_hours,:]\n",
        "      test_y[i] = test[i+n_hours,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n",
        "def create_data3(n_records = 7, n_train = 10000, n_test = -1, stride =24):\n",
        "  n_train+=n_records*stride\n",
        "  if n_test > 0:\n",
        "    n_test+=n_records*stride\n",
        "  \n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = n_test if n_test > 0 else data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "  n_items = n_records*stride\n",
        "\n",
        "  train_X = np.zeros([n_train-n_items, n_records, 8])\n",
        "  train_y = np.zeros([n_train-n_items, 1])\n",
        "  \n",
        "  \n",
        "  for i in range (n_train-n_items):\n",
        "      train_X[i,:,:] = train[i:i+n_items:stride,:]\n",
        "      train_y[i] = train[i+n_items,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_items, n_records, 8])\n",
        "  test_y = np.zeros([n_test-n_items, 1])\n",
        "  for i in range (n_test-n_items):\n",
        "      test_X[i,:,:] = test[i:i+n_items:stride,:]\n",
        "      test_y[i] = test[i+n_items,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLRasYy_RTgT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "eZ_fxxL6xmSP",
        "colab_type": "code",
        "outputId": "c3b79f20-39e0-4672-abcd-dcd19e7ebec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Concatenate, Input, concatenate\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU Processing\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at:{}'.format(device_name))\n",
        "n_train = 10000\n",
        "n_test = 20000\n",
        "batch_size=100\n",
        "\n",
        "# train_X,train_y, test_X, test_y = create_data2(n_hours, n_train=n_train)\n",
        "\n",
        "train_X_1,train_y, test_X_1, test_y = create_data3(n_records=24, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=1)\n",
        "train_X_2,train_y, test_X_2, test_y = create_data3(n_records=4, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=2)\n",
        "\n",
        "train_X_3,train_y, test_X_3, test_y = create_data3(n_records=4, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=12)\n",
        "\n",
        "\n",
        "inp1 = Input(shape=(train_X_1.shape[1], train_X_1.shape[2]))\n",
        "inp2 = Input(shape=(train_X_2.shape[1], train_X_2.shape[2]))\n",
        "inp3 = Input(shape=(train_X_3.shape[1], train_X_3.shape[2]))\n",
        " \n",
        "\n",
        "\n",
        "x = LSTM(24)(inp1)\n",
        "x = Dense(1)(x)\n",
        "\n",
        "y = LSTM(24)(inp2)\n",
        "y = Dense(1)(y)\n",
        "\n",
        "z = LSTM(24)(inp3)\n",
        "z = Dense(1)(z)\n",
        "\n",
        "w = concatenate([x, y, z])\n",
        "\n",
        "out =  Dense(1, activation='linear')(w)\n",
        "\n",
        "model = Model(inputs=[inp1, inp2, inp3], outputs=out)\n",
        "\n",
        "model.compile(loss='mse',\n",
        "        optimizer='adam',\n",
        "        metrics=['mae'])\n",
        "\n",
        "legends =['training_loss', 'validation_loss']\n",
        "\n",
        "print(f\"############ {model.name} ########################\")\n",
        "\n",
        "history = model.fit([train_X_1, train_X_2, train_X_3], train_y, \n",
        "                    epochs=25,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1)\n",
        "# plot history\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training\")\n",
        "plt.legend(legends)\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Test\")\n",
        "plt.legend(legends)\n",
        "\n",
        "\n",
        "test_loss = model.evaluate([test_X_1, test_X_2, test_X_3], test_y)\n",
        "\n",
        "print(\"test loss:\",test_loss)\n",
        "\n",
        "test_predictions = model.predict([test_X_1, test_X_2, test_X_3]).flatten()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(num=1, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title(f\"Prediction of {model.name}: Test loss (MSE, MAE):{test_loss}\")\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('normalized value')\n",
        "plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "plt.plot(data[:,0], 'bo')\n",
        "plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go', alpha=0.4)\n",
        "plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at:/device:GPU:0\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 24, 8) (10000, 1) (20000, 24, 8) (20000, 1)\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 4, 8) (10000, 1) (20000, 4, 8) (20000, 1)\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 4, 8) (10000, 1) (20000, 4, 8) (20000, 1)\n",
            "############ model_5 ########################\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/25\n",
            " 160/9000 [..............................] - ETA: 3:00 - loss: 0.0174 - mean_absolute_error: 0.0995"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TPwH-yrYBQrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "\n",
        "|hours|Mean Absolute Error |  loss function \n",
        "|--|--|\n",
        "|1  |  0.01328097059825935\n",
        "|10 |   0.016554366017757748 \n",
        "| 24 | 0.02725420025859075 \n",
        "| 50|  0.023748632413918377\n",
        "| 50 |    0.03477914220943476 | MSE\n",
        "| 1 |  0.042994678616862125\n",
        "| 1 |  0.016500973061146625\n",
        "| 48| 0.02513334271283482 \n",
        "|48| 0.01888059062355331\n",
        "|72| 0.020766058881570845\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cBWrhC9hRS4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "f809b69b-710d-4a92-ec31-1d56ca65fa46"
      },
      "cell_type": "code",
      "source": [
        "|# import pandas as pd\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "# y_train = train_y\n",
        "# x_train = train_X\n",
        "\n",
        "# y_test = test_y\n",
        "# x_test = test_X\n",
        "\n",
        "# EPOCHS = 10\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(120, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(18, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(1, activation='linear')\n",
        "# ])\n",
        "\n",
        "# optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "# model.compile(loss='mse',\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=['mae'])\n",
        "\n",
        "\n",
        "# # Store training stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# def plot_history(history):\n",
        "#     plt.figure()\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Mean Abs Error ')\n",
        "#     plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "#              label='Train Loss')\n",
        "#     plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "#              label='Val loss')\n",
        "#     plt.legend()\n",
        "#     #plt.ylim([0, 0.2])\n",
        "\n",
        "\n",
        "# #model.summary()\n",
        "# history = model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "#                 validation_split=0.1, verbose=1)\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# plot_history(history)\n",
        "# test_predictions = model.predict(x_test).flatten()\n",
        "# test_acc, test_loss = model.evaluate(x_test, y_test)\n",
        "# print(f\"test accuracy: {test_acc}, test loss {test_loss}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d37156ee0b35>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    |# import pandas as pd\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IUwrIvafdh4K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Plot Predicted Values"
      ]
    },
    {
      "metadata": {
        "id": "WKBAT0RFZCDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(num=None, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('normalized value')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "# plt.plot(data[:,0], 'bo')\n",
        "# plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}