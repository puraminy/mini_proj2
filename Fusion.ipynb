{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fusion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj2/blob/master/Fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2W_bevOEYdEv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load DATA\n"
      ]
    },
    {
      "metadata": {
        "id": "OrhkJ3uO5lD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data Main Source\n",
        "#####https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data"
      ]
    },
    {
      "metadata": {
        "id": "_RE5WNDFmq-T",
        "colab_type": "code",
        "outputId": "fc3f0bfe-ce43-45a7-d05b-d65b9829e10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/puraminy/mini_proj2\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mini_proj2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyxKkShP1XJg",
        "colab_type": "code",
        "outputId": "ef6e2d4c-7c69-4f53-c3d9-b4cb1b94744d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
        "\n",
        "#github = \"https://raw.githubusercontent.com/puraminy/mini_proj2/master/polution.csv\"\n",
        "\n",
        "github = \"mini_proj2/polution.csv\"\n",
        "\n",
        "c=pd.read_csv(github)\n",
        "data=np.asarray(c)\n",
        "print(data)\n",
        "np.shape(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.12977867 0.35294122 0.24590163 ... 0.00229001 0.         0.        ]\n",
            " [0.14889336 0.36764708 0.24590163 ... 0.00381099 0.         0.        ]\n",
            " [0.15995975 0.42647061 0.22950819 ... 0.00533197 0.         0.        ]\n",
            " ...\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.40558836 0.         0.        ]\n",
            " [0.01006036 0.2647059  0.26229507 ... 0.41399646 0.         0.        ]\n",
            " [0.00804829 0.2647059  0.24590163 ... 0.42086649 0.         0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43799, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "kUWTG2AjYoJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "a4ZQnQPLYpOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "values = c.values\n",
        "# specify columns to plot\n",
        "def plot_trends():\n",
        "  groups = [0, 1, 2, 3, 5, 6, 7]\n",
        "  i = 1\n",
        "  # plot each column\n",
        "  pyplot.figure()\n",
        "  for group in groups:\n",
        "    pyplot.subplot(len(groups), 1, i)\n",
        "    pyplot.plot(values[:, group])\n",
        "    pyplot.title(c.columns[group], y=0.5, loc='right')\n",
        "    i += 1\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZmb0Vi6v3Y2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing Time Series"
      ]
    },
    {
      "metadata": {
        "id": "xKQRfONIv-Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "# load dataset\n",
        "def create_data(n_hours = 24, n_train = 10000):\n",
        "  dataset = read_csv('mini_proj2/polution.csv', header=0)\n",
        "  values = dataset.values\n",
        "  \n",
        "  print(np.shape(values))\n",
        "  # integer encode direction\n",
        "  encoder = LabelEncoder()\n",
        "  values[:,4] = encoder.fit_transform(values[:,4])\n",
        "  # ensure all data is float\n",
        "  values = values.astype('float32')\n",
        "  # normalize features\n",
        "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  scaled = values # scaler.fit_transform(values)\n",
        "  # specify the number of lag hours\n",
        "  # n_hours = 24\n",
        "  n_features = 8 \n",
        "  # frame as supervised learning\n",
        "  reframed = series_to_supervised(scaled, n_hours, 1)\n",
        "\n",
        " # print(reframed[:2])\n",
        " # print(reframed.shape)\n",
        "\n",
        "  # split into train and test sets\n",
        "  values = reframed.values\n",
        "   #365 * 24\n",
        "  train = values[:n_train, :]\n",
        "  test = values[n_train:, :]\n",
        "  # split into input and outputs\n",
        "  n_obs = n_hours * n_features\n",
        "  train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
        "\n",
        "  test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
        "  print(train_X.shape, len(train_X), train_y.shape)\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "    \n",
        "  train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "  test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B99o1xwiwadG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Train & Test Data"
      ]
    },
    {
      "metadata": {
        "id": "PVs3NEqA_SOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_data2(n_hours = 24, n_train = 10000, step =1):\n",
        "  n_train+=n_hours\n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = 1000 #data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "\n",
        "  train_X = np.zeros([n_train-n_hours, n_hours, 8])\n",
        "  train_y = np.zeros([n_train-n_hours, 1])\n",
        "  for i in range (n_train-n_hours):\n",
        "      train_X[i,:,:] = train[i:i+n_hours,:]\n",
        "      train_y[i] = train[i+n_hours,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_hours, n_hours, 8])\n",
        "  test_y = np.zeros([n_test-n_hours, 1])\n",
        "  for i in range (n_test-n_hours):\n",
        "      test_X[i,:,:] = test[i:i+n_hours,:]\n",
        "      test_y[i] = test[i+n_hours,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n",
        "def create_data3(n_records = 7, n_train = 10000, n_test = -1, stride =24):\n",
        "  n_train+=n_records*stride\n",
        "  if n_test > 0:\n",
        "    n_test+=n_records*stride\n",
        "  \n",
        "  c=pd.read_csv(github)\n",
        "  data=np.asarray(c)\n",
        "\n",
        "  print(np.shape(data))\n",
        "  \n",
        "  print(data.shape[0])\n",
        "  n_test = n_test if n_test > 0 else data.shape[0]-n_train\n",
        "  train = data[:n_train, :]\n",
        "  test = data[n_train:n_train+n_test, :]\n",
        "\n",
        "  n_items = n_records*stride\n",
        "\n",
        "  train_X = np.zeros([n_train-n_items, n_records, 8])\n",
        "  train_y = np.zeros([n_train-n_items, 1])\n",
        "  \n",
        "  \n",
        "  for i in range (n_train-n_items):\n",
        "      train_X[i,:,:] = train[i:i+n_items:stride,:]\n",
        "      train_y[i] = train[i+n_items,0]\n",
        "\n",
        "  test_X = np.zeros([n_test-n_items, n_records, 8])\n",
        "  test_y = np.zeros([n_test-n_items, 1])\n",
        "  for i in range (n_test-n_items):\n",
        "      test_X[i,:,:] = test[i:i+n_items:stride,:]\n",
        "      test_y[i] = test[i+n_items,0]\n",
        "\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  \n",
        "  return train_X,train_y, test_X, test_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLRasYy_RTgT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "eZ_fxxL6xmSP",
        "colab_type": "code",
        "outputId": "193efc36-c2b2-47b1-edab-945f3f6546a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Concatenate, Input, concatenate\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU Processing\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at:{}'.format(device_name))\n",
        "\n",
        "def fusion_model(left_X, right_X, optimizer):\n",
        " \n",
        "  left = Sequential()\n",
        "  left.add(LSTM(units=24,input_shape=(left_X.shape[1], left_X.shape[2])))\n",
        "  left.add(Dense(1))\n",
        "\n",
        "  right = Sequential()\n",
        "  right.add(LSTM(units=24,input_shape=(right_X.shape[1], right_X.shape[2])))\n",
        "  right.add(Dense(1))\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Concatenate([left,right]))  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "#   model.fit([X_left, X_right], Y_train, nb_epoch=100,validation_split=0.2,show_accuracy=True)\n",
        "\n",
        "#   left = Sequential()   \n",
        "#   left.add(LSTM(24, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "#   left.add(Dense(1))\n",
        "\n",
        "#   model = Sequential()\n",
        "#   model.add(Concatenate([left,right]))\n",
        "#   model.add(Dense(1, activation='linear'))\n",
        "  \n",
        "#   model = Model(inputs=[x, y], outputs=output)\n",
        "  \n",
        "  model.name = 'fusion_lstms'\n",
        "  if optimizer == 'rmsprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_model(cell_type = \"normal\", optimizer = 'adam', dropout=False, train_X=None):\n",
        "  if cell_type == \"lstm\":\n",
        "    model = Sequential()   \n",
        "    if not dropout:   \n",
        "      model.add(LSTM(24, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    else:\n",
        "#       model.add(Dropout(0.1)) \n",
        "      model.add(LSTM(24, batch_input_shape=(100, train_X.shape[1], train_X.shape[2]), \n",
        "                     stateful=True, \n",
        "                     dropout=0.2))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  elif cell_type == \"gru\":\n",
        "    model = Sequential()\n",
        "    model.add(GRU(30, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  elif cell_type == \"rnn\":\n",
        "    model = Sequential()\n",
        "    if not dropout:\n",
        "      model.add(SimpleRNN(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    else:\n",
        "      model.add(SimpleRNN(100, batch_input_shape=(100, train_X.shape[1], train_X.shape[2]), \n",
        "                     stateful=True, \n",
        "                     dropout=0.1))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  else:\n",
        "    model = Sequential()\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10))\n",
        "#     model.add(Dense(20)) \n",
        "    model.add(Dense(10)) \n",
        "    model.add(Dense(1, activation='linear'))\n",
        "  \n",
        "  model.name = 'model_' + cell_type + '_' + optimizer + ('_dropout' if dropout else '')\n",
        "  if optimizer == 'rmsprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "    \n",
        "#   model.compile(loss='mae', optimizer=optimizer)\n",
        "  \n",
        "  model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "n_train = 10000\n",
        "n_test = 25000\n",
        "n_hours = 24\n",
        "batch_size=100\n",
        "\n",
        "# train_X,train_y, test_X, test_y = create_data2(n_hours, n_train=n_train)\n",
        "\n",
        "train_X_left,train_y, test_X_left, test_y = create_data3(n_records=24, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=1)\n",
        "train_X_right,train_y, test_X_right, test_y = create_data3(n_records=24, \n",
        "                                               n_train=n_train,\n",
        "                                               n_test=n_test,\n",
        "                                               stride=4)\n",
        "\n",
        "\n",
        "# model_normal_adam = create_model(optimizer='adam') #0\n",
        "# model_normal_sgd = create_model(optimizer='sgd') #1\n",
        "# model_normal_rmsprop = create_model(optimizer='rmsprop') #2\n",
        "\n",
        "# model_lstm_adam = create_model(\"lstm\", 'adam', train_X= train_X_left) #3\n",
        "# model_lstm_sgd = create_model(\"lstm\",'sgd') #4\n",
        "# model_lstm_rmsprop = create_model(\"lstm\",'rmsprop') #5\n",
        "\n",
        "\n",
        "# model_gru_adam = create_model(\"gru\", 'adam', train_X= train_X_right) #6\n",
        "# model_gru_sgd = create_model(\"gru\", 'sgd') #7\n",
        "# model_gru_rmsprop = create_model(\"gru\", 'rmsprop') #8\n",
        "\n",
        "\n",
        "# model_rnn_adam = create_model(\"rnn\", 'adam') #9\n",
        "# model_rnn_sgd = create_model(\"rnn\", 'sgd') #10\n",
        "# model_rnn_rmsprop = create_model(\"rnn\", 'rmsprop') #11\n",
        "\n",
        "# model_lstm_adam_dropout = create_model(\"lstm\", 'adam',dropout=True) #12\n",
        "# model_rnn_adam_dropout = create_model(\"rnn\", 'adam',dropout=True) #13\n",
        "\n",
        "\n",
        "\n",
        "# model = fusion_model(train_X_left, train_X_right, 'adam')\n",
        "\n",
        "# left = Sequential()\n",
        "# left.add(LSTM(units=24,input_shape=(train_X_left.shape[1], train_X_left.shape[2])))\n",
        "# left.add(Dense(1))\n",
        "\n",
        "# right = Sequential()\n",
        "# right.add(LSTM(units=24,input_shape=(train_X_right.shape[1], train_X_right.shape[2])))\n",
        "# right.add(Dense(1))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Concatenate([left,right]))  \n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# model.compile(loss='mse',\n",
        "#         optimizer='adam',\n",
        "#         metrics=['mae'])\n",
        "\n",
        "# history = model.fit(concatenate([train_X_left, train_X_right]), train_y, \n",
        "#               epochs=40,\n",
        "#               validation_split=0.2,\n",
        "#               verbose=1)\n",
        "\n",
        "\n",
        "inp1 = Input(shape=(train_X_left.shape[1], train_X_left.shape[2]))\n",
        "inp2 = Input(shape=(train_X_right.shape[1], train_X_right.shape[2]))\n",
        " \n",
        "\n",
        "\n",
        "x = LSTM(24)(inp1)\n",
        "x = Dense(1)(x)\n",
        "\n",
        "y = LSTM(24)(inp2)\n",
        "y = Dense(1)(y)\n",
        "\n",
        "z = concatenate([x, y])\n",
        "\n",
        "out =  Dense(1, activation='linear')(z)\n",
        "\n",
        "model = Model(inputs=[inp1, inp2], outputs=out)\n",
        "\n",
        "model.compile(loss='mse',\n",
        "        optimizer='adam',\n",
        "        metrics=['mae'])\n",
        "\n",
        "legends =[]\n",
        "\n",
        "print(f\"############ {model.name} ########################\")\n",
        "\n",
        "history = model.fit([train_X_left, train_X_right], train_y, \n",
        "                    epochs=10,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1)\n",
        "\n",
        "legends.append(model.name)\n",
        "# plot history\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training\")\n",
        "plt.legend(legends)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Test\")\n",
        "plt.legend(legends)\n",
        "\n",
        "\n",
        "test_loss = model.evaluate([test_X_left, test_X_right], test_y,batch_size=batch_size)\n",
        "\n",
        "print(\"test loss:\",test_loss)\n",
        "\n",
        "test_predictions = model.predict([test_X_left, test_X_right],batch_size=batch_size).flatten()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(num=1, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title(f\"Prediction of {model.name}\")\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('normalized value')\n",
        "plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "plt.plot(data[:,0], 'bo')\n",
        "plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go', alpha=0.4)\n",
        "plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at:/device:GPU:0\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 24, 8) (10000, 1) (25000, 24, 8) (25000, 1)\n",
            "(43799, 8)\n",
            "43799\n",
            "(10000, 24, 8) (10000, 1) (25000, 24, 8) (25000, 1)\n",
            "############ model_10 ########################\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/10\n",
            "2560/8000 [========>.....................] - ETA: 30s - loss: 0.0104 - mean_absolute_error: 0.0756"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TPwH-yrYBQrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "\n",
        "|hours|Mean Absolute Error |  loss function \n",
        "|--|--|\n",
        "|1  |  0.01328097059825935\n",
        "|10 |   0.016554366017757748 \n",
        "| 24 | 0.02725420025859075 \n",
        "| 50|  0.023748632413918377\n",
        "| 50 |    0.03477914220943476 | MSE\n",
        "| 1 |  0.042994678616862125\n",
        "| 1 |  0.016500973061146625\n",
        "| 48| 0.02513334271283482 \n",
        "|48| 0.01888059062355331\n",
        "|72| 0.020766058881570845\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cBWrhC9hRS4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "|# import pandas as pd\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "# y_train = train_y\n",
        "# x_train = train_X\n",
        "\n",
        "# y_test = test_y\n",
        "# x_test = test_X\n",
        "\n",
        "# EPOCHS = 10\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(120, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(18, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(1, activation='linear')\n",
        "# ])\n",
        "\n",
        "# optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "# model.compile(loss='mse',\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=['mae'])\n",
        "\n",
        "\n",
        "# # Store training stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# def plot_history(history):\n",
        "#     plt.figure()\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Mean Abs Error ')\n",
        "#     plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "#              label='Train Loss')\n",
        "#     plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "#              label='Val loss')\n",
        "#     plt.legend()\n",
        "#     #plt.ylim([0, 0.2])\n",
        "\n",
        "\n",
        "# #model.summary()\n",
        "# history = model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "#                 validation_split=0.1, verbose=1)\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# plot_history(history)\n",
        "# test_predictions = model.predict(x_test).flatten()\n",
        "# test_acc, test_loss = model.evaluate(x_test, y_test)\n",
        "# print(f\"test accuracy: {test_acc}, test loss {test_loss}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUwrIvafdh4K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Plot Predicted Values"
      ]
    },
    {
      "metadata": {
        "id": "WKBAT0RFZCDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(num=None, figsize=(20, 3), dpi=80, facecolor='w', edgecolor='k')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('normalized value')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n",
        "# plt.plot(data[:,0], 'bo')\n",
        "# plt.plot(np.concatenate([data[:n_train,0],test_predictions]), 'go')\n",
        "# plt.legend(('Original', 'Predicted'), loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}